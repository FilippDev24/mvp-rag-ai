#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π HTTP —Å–µ—Ä–≤–µ—Ä –¥–ª—è gpt-oss-20b —á–µ—Ä–µ–∑ Transformers
–°–æ–≤–º–µ—Å—Ç–∏–º —Å OpenAI API –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from flask import Flask, request, jsonify
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
model = None
tokenizer = None

def load_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ gpt-oss-20b"""
    global model, tokenizer
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    tokenizer_path = "openai/gpt-oss-20b"
    model_path = "/tmp/gpt-oss-20b/original"
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ {tokenizer_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_path,
        trust_remote_code=True,
        use_fast=False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    )
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ {model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    logger.info(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {model.device}")
    logger.info(f"–ü–∞–º—è—Ç—å GPU: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

@app.route('/v1/models', methods=['GET'])
def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    return jsonify({
        "object": "list",
        "data": [
            {
                "id": "openai/gpt-oss-20b",
                "object": "model",
                "created": 1234567890,
                "owned_by": "openai"
            }
        ]
    })

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    """OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π endpoint –¥–ª—è —á–∞—Ç–∞"""
    try:
        data = request.json
        messages = data.get('messages', [])
        max_tokens = data.get('max_tokens', 512)
        temperature = data.get('temperature', 0.7)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π
        prompt = ""
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            if role == 'user':
                prompt += f"User: {content}\n"
            elif role == 'assistant':
                prompt += f"Assistant: {content}\n"
        
        prompt += "Assistant:"
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        
        return jsonify({
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1234567890,
            "model": "openai/gpt-oss-20b",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response.strip()
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": inputs.input_ids.shape[1],
                "completion_tokens": len(tokenizer.encode(response)),
                "total_tokens": inputs.input_ids.shape[1] + len(tokenizer.encode(response))
            }
        })
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    """Health check"""
    return jsonify({"status": "ok", "model_loaded": model is not None})

if __name__ == '__main__':
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ gpt-oss-20b")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    load_model()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä
    app.run(host='0.0.0.0', port=8000, debug=False)
