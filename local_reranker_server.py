#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ–¥ macOS (Apple Silicon) –∏ Ubuntu (x86_64)
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ù–ê –•–û–°–¢–ï (–Ω–µ –≤ Docker) –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ü–û–†–¢: 8002 (—á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏)
"""

import os
import platform
import logging
import time
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from sentence_transformers import CrossEncoder
import numpy as np
import uvicorn

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
PLATFORM = platform.system()
ARCHITECTURE = platform.machine()
IS_APPLE_SILICON = PLATFORM == "Darwin" and ARCHITECTURE == "arm64"
IS_UBUNTU = PLATFORM == "Linux"

logger.info(f"üñ•Ô∏è  Platform: {PLATFORM} {ARCHITECTURE}")
if IS_APPLE_SILICON:
    logger.info("üçé Detected Apple Silicon - will use MPS optimizations")
elif IS_UBUNTU:
    logger.info("üêß Detected Ubuntu - will use CUDA/CPU optimizations")

# Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è API
class RerankRequest(BaseModel):
    query: str
    documents: List[str]
    top_k: int = 10

class RerankResponse(BaseModel):
    results: List[Dict[str, Any]]
    processing_time_ms: float
    device_used: str

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
class RerankConfig:
    MODEL = os.getenv('RERANKER_MODEL', 'BAAI/bge-reranker-v2-m3')
    MAX_LENGTH = int(os.getenv('RERANKER_MAX_LENGTH', '512'))

# FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title=f"Universal Reranker Service ({PLATFORM})",
    description=f"High-performance reranking service for {PLATFORM} {ARCHITECTURE}",
    version="1.0.0"
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
model = None
device = None
config = RerankConfig()

def setup_device_optimizations():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã"""
    global device
    
    if IS_APPLE_SILICON:
        # Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if torch.backends.mps.is_available():
            device = "mps"
            logger.info("üöÄ Using MPS (Metal Performance Shaders) for Apple Silicon")
        else:
            device = "cpu"
            logger.info("‚ö†Ô∏è  MPS not available, using CPU")
            
    elif IS_UBUNTU:
        # Ubuntu –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if torch.cuda.is_available():
            device = "cuda"
            logger.info("üöÄ Using CUDA GPU on Ubuntu")
            # CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            if hasattr(torch.backends.cuda, 'matmul'):
                torch.backends.cuda.matmul.allow_tf32 = True
            if hasattr(torch.backends.cudnn, 'allow_tf32'):
                torch.backends.cudnn.allow_tf32 = True
        else:
            device = "cpu"
            logger.info("‚ö†Ô∏è  CUDA not available, using CPU")
            # CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Ubuntu
            torch.set_num_threads(os.cpu_count())
            os.environ['OMP_NUM_THREADS'] = str(os.cpu_count())
            os.environ['MKL_NUM_THREADS'] = str(os.cpu_count())
    else:
        # Fallback –¥–ª—è –¥—Ä—É–≥–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üîß Using {device} on {PLATFORM}")

def clear_device_cache():
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã"""
    if device == "mps" and IS_APPLE_SILICON:
        if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
            torch.mps.empty_cache()
        elif hasattr(torch.backends.mps, 'empty_cache'):
            torch.backends.mps.empty_cache()
    elif device == "cuda":
        torch.cuda.empty_cache()

def initialize_model():
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
    global model
    
    try:
        setup_device_optimizations()
        
        logger.info(f"üì• Loading reranker model: {config.MODEL}")
        start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É—á–µ—Ç–æ–º –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
        model = CrossEncoder(
            config.MODEL,
            max_length=config.MAX_LENGTH,
            device=device
        )
        
        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
        clear_device_cache()
        
        load_time = time.time() - start_time
        logger.info(f"‚úÖ Model loaded successfully in {load_time:.2f} seconds")
        
        # –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏
        logger.info("üî• Warming up reranker model...")
        warmup_start = time.time()
        _ = model.predict([("test query", "test document")])
        warmup_time = time.time() - warmup_start
        logger.info(f"‚úÖ Model warmup completed in {warmup_time:.2f} seconds")
        logger.info(f"üéØ Ready for high-performance reranking on {PLATFORM}!")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize model: {str(e)}")
        raise e

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞"""
    logger.info(f"üöÄ Starting Universal Reranker Server on {PLATFORM} (port 8002)...")
    initialize_model()

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "device": device,
        "platform": PLATFORM,
        "architecture": ARCHITECTURE,
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "mps_available": torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
        "port": 8002,
        "service": "universal_reranker",
        "model_name": config.MODEL
    }

@app.post("/rerank", response_model=RerankResponse)
async def rerank_documents(request: RerankRequest):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    if model is None:
        raise HTTPException(status_code=500, detail="Model not initialized")
    
    if not request.documents:
        return RerankResponse(
            results=[],
            processing_time_ms=0.0,
            device_used=device
        )
    
    try:
        start_time = time.time()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä (query, document)
        pairs = [(request.query, doc) for doc in request.documents]
        
        # –†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω—ã–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
        with torch.no_grad():
            clear_device_cache()
            scores = model.predict(pairs)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫–æ—Ä–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É, —á—Ç–æ –∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
        scores_array = np.array(scores)
        
        # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–∏–π
        amplification_factor = 100
        amplified_scores = np.exp(scores_array * amplification_factor)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω 0-10
        min_amplified = np.min(amplified_scores)
        max_amplified = np.max(amplified_scores)
        
        if max_amplified > min_amplified:
            final_scores = (amplified_scores - min_amplified) / (max_amplified - min_amplified) * 10
        else:
            final_scores = np.ones_like(amplified_scores) * 5.0
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = []
        for i, (raw_score, final_score) in enumerate(zip(scores_array, final_scores)):
            results.append({
                "index": i,
                "score": float(final_score),
                "raw_logit": float(raw_score),
                "document": request.documents[i]
            })
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–∫–æ—Ä–∞
        results.sort(key=lambda x: x["score"], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—Ç —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_results = results[:request.top_k]
        
        processing_time = (time.time() - start_time) * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
        
        logger.info(f"‚ö° Reranked {len(request.documents)} documents in {processing_time:.1f}ms on {device}")
        
        return RerankResponse(
            results=top_results,
            processing_time_ms=processing_time,
            device_used=device
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error during reranking: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Reranking failed: {str(e)}")

@app.get("/model-info")
async def get_model_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    expected_perf = "100-300ms on Apple Silicon M4 Pro" if IS_APPLE_SILICON else "200-800ms on Ubuntu (depends on hardware)"
    
    return {
        "model_name": config.MODEL,
        "max_length": config.MAX_LENGTH,
        "device": device,
        "platform": PLATFORM,
        "architecture": ARCHITECTURE,
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "mps_available": torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
        "port": 8002,
        "expected_performance": expected_perf
    }

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint"""
    return {
        "service": f"Universal Reranker Server ({PLATFORM})",
        "status": "running",
        "port": 8002,
        "model": config.MODEL,
        "platform": f"{PLATFORM} {ARCHITECTURE}",
        "endpoints": {
            "health": "/health",
            "rerank": "/rerank",
            "model_info": "/model-info"
        }
    }

if __name__ == "__main__":
    print(f"üöÄ Starting Universal Reranker Server on {PLATFORM}...")
    print("üìç Port: 8002")
    print(f"ü§ñ Model: {config.MODEL}")
    print(f"üéØ Optimized for {PLATFORM} {ARCHITECTURE}")
    
    # –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
    uvicorn.run(
        "local_reranker_server:app",
        host="0.0.0.0",  # –°–ª—É—à–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞—Ö
        port=8002,
        log_level="info",
        reload=False
    )
