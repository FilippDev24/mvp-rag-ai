#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ–¥ macOS (Apple Silicon) –∏ Ubuntu (x86_64)
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ù–ê –•–û–°–¢–ï (–Ω–µ –≤ Docker) –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ü–û–†–¢: 8003 (—á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏)
"""

import os
import platform
import logging
import time
import re
from typing import List, Dict, Any, Tuple, Optional
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
import numpy as np
import uvicorn

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
PLATFORM = platform.system()
ARCHITECTURE = platform.machine()
IS_APPLE_SILICON = PLATFORM == "Darwin" and ARCHITECTURE == "arm64"
IS_UBUNTU = PLATFORM == "Linux"

logger.info(f"üñ•Ô∏è  Platform: {PLATFORM} {ARCHITECTURE}")
if IS_APPLE_SILICON:
    logger.info("üçé Detected Apple Silicon - will use MPS optimizations")
elif IS_UBUNTU:
    logger.info("üêß Detected Ubuntu - will use CUDA/CPU optimizations")

# Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è API
class EmbeddingRequest(BaseModel):
    texts: List[str]
    is_query: bool = False

class BatchEmbeddingRequest(BaseModel):
    texts: List[str]
    is_query: bool = False
    batch_size: int = 32

class EmbeddingResponse(BaseModel):
    embeddings: List[List[float]]
    processing_time_ms: float
    device_used: str
    total_tokens: int
    model_info: Dict[str, Any]

class SingleEmbeddingRequest(BaseModel):
    text: str
    is_query: bool = False

class SingleEmbeddingResponse(BaseModel):
    embedding: List[float]
    processing_time_ms: float
    device_used: str
    tokens: int
    detected_language: Optional[str] = None
    instruction_prefix: Optional[str] = None

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó)
class EmbeddingConfig:
    # –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    MODEL = os.getenv('EMBEDDING_MODEL', 'intfloat/multilingual-e5-large-instruct')
    DIMENSION = int(os.getenv('EMBEDDING_DIMENSION', '1024'))
    MAX_SEQ_LENGTH = int(os.getenv('EMBEDDING_MAX_SEQ_LENGTH', '512'))
    BATCH_SIZE = int(os.getenv('EMBEDDING_BATCH_SIZE', '32'))
    
    # –ü—Ä–µ—Ñ–∏–∫—Å—ã –¥–ª—è instruct –º–æ–¥–µ–ª–∏
    DOCUMENT_PREFIX = ""
    QUERY_PREFIX_RU = "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –ù–∞–π–¥–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n–ó–∞–ø—Ä–æ—Å: "
    QUERY_PREFIX_EN = "Instruct: Given a search query, retrieve relevant passages from knowledge base\nQuery: "

# FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title=f"Universal Embedding Service ({PLATFORM})",
    description=f"High-performance embedding service for {PLATFORM} {ARCHITECTURE}",
    version="1.0.0"
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
model = None
tokenizer = None
device = None
config = EmbeddingConfig()

def detect_language(text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –ø–æ –Ω–∞–ª–∏—á–∏—é –∫–∏—Ä–∏–ª–ª–∏—Ü—ã"""
    cyrillic_chars = len(re.findall(r'[–∞-—è—ë]', text.lower()))
    total_chars = len(re.findall(r'[–∞-—è—ëa-z]', text.lower()))
    
    if total_chars == 0:
        return 'en'
    
    cyrillic_ratio = cyrillic_chars / total_chars
    return 'ru' if cyrillic_ratio > 0.3 else 'en'

def get_query_prefix(query: str) -> Tuple[str, str]:
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–∞"""
    detected_language = detect_language(query)
    
    if detected_language == 'ru':
        return config.QUERY_PREFIX_RU, 'ru'
    else:
        return config.QUERY_PREFIX_EN, 'en'

def setup_device_optimizations():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã"""
    global device
    
    if IS_APPLE_SILICON:
        # Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if torch.backends.mps.is_available():
            device = "mps"
            logger.info("üöÄ Using MPS (Metal Performance Shaders) for Apple Silicon")
        else:
            device = "cpu"
            logger.info("‚ö†Ô∏è  MPS not available, using CPU")
            
    elif IS_UBUNTU:
        # Ubuntu –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if torch.cuda.is_available():
            device = "cuda"
            logger.info("üöÄ Using CUDA GPU on Ubuntu")
            # CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            if hasattr(torch.backends.cuda, 'matmul'):
                torch.backends.cuda.matmul.allow_tf32 = True
            if hasattr(torch.backends.cudnn, 'allow_tf32'):
                torch.backends.cudnn.allow_tf32 = True
        else:
            device = "cpu"
            logger.info("‚ö†Ô∏è  CUDA not available, using CPU")
            # CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Ubuntu
            torch.set_num_threads(os.cpu_count())
            os.environ['OMP_NUM_THREADS'] = str(os.cpu_count())
            os.environ['MKL_NUM_THREADS'] = str(os.cpu_count())
    else:
        # Fallback –¥–ª—è –¥—Ä—É–≥–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üîß Using {device} on {PLATFORM}")

def clear_device_cache():
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã"""
    if device == "mps" and IS_APPLE_SILICON:
        if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):
            torch.mps.empty_cache()
        elif hasattr(torch.backends.mps, 'empty_cache'):
            torch.backends.mps.empty_cache()
    elif device == "cuda":
        torch.cuda.empty_cache()

def initialize_model():
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
    global model, tokenizer
    
    try:
        setup_device_optimizations()
        
        logger.info(f"üì• Loading embedding model: {config.MODEL}")
        start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É—á–µ—Ç–æ–º –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
        model_device = device if device != "mps" else "cpu"  # SentenceTransformer –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å MPS –Ω–∞–ø—Ä—è–º—É—é
        
        model = SentenceTransformer(
            config.MODEL,
            device=model_device
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
        model.max_seq_length = config.MAX_SEQ_LENGTH
        model.eval()  # –†–µ–∂–∏–º inference
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        tokenizer = AutoTokenizer.from_pretrained(config.MODEL)
        
        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
        clear_device_cache()
        
        load_time = time.time() - start_time
        logger.info(f"‚úÖ Model loaded successfully in {load_time:.2f} seconds")
        
        # –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏
        logger.info("üî• Warming up embedding model...")
        warmup_start = time.time()
        _ = model.encode("test document", normalize_embeddings=True)
        warmup_time = time.time() - warmup_start
        logger.info(f"‚úÖ Model warmup completed in {warmup_time:.2f} seconds")
        logger.info(f"üéØ Ready for high-performance embedding generation on {PLATFORM}!")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize model: {str(e)}")
        raise e

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞"""
    logger.info(f"üöÄ Starting Universal Embedding Server on {PLATFORM} (port 8003)...")
    initialize_model()

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "device": device,
        "platform": PLATFORM,
        "architecture": ARCHITECTURE,
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "mps_available": torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
        "port": 8003,
        "service": "universal_embedding",
        "model_name": config.MODEL,
        "dimension": config.DIMENSION
    }

@app.post("/embed", response_model=SingleEmbeddingResponse)
async def generate_single_embedding(request: SingleEmbeddingRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="Model not initialized")
    
    try:
        start_time = time.time()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
        if request.is_query:
            query_prefix, detected_language = get_query_prefix(request.text)
            prefixed_text = query_prefix + request.text
            instruction_prefix = query_prefix.split('\n')[0]
        else:
            prefixed_text = config.DOCUMENT_PREFIX + request.text
            detected_language = None
            instruction_prefix = None
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
        tokens = len(tokenizer.encode(prefixed_text, truncation=True, max_length=config.MAX_SEQ_LENGTH))
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        with torch.no_grad():
            clear_device_cache()
            
            embedding = model.encode(
                prefixed_text,
                batch_size=1,
                normalize_embeddings=True,
                show_progress_bar=False,
                convert_to_numpy=True
            )
        
        processing_time = (time.time() - start_time) * 1000
        
        logger.info(f"‚ö° Generated embedding in {processing_time:.1f}ms ({tokens} tokens) on {device}")
        
        return SingleEmbeddingResponse(
            embedding=embedding.tolist(),
            processing_time_ms=processing_time,
            device_used=device,
            tokens=tokens,
            detected_language=detected_language,
            instruction_prefix=instruction_prefix
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error generating embedding: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Embedding generation failed: {str(e)}")

@app.post("/embed-batch", response_model=EmbeddingResponse)
async def generate_batch_embeddings(request: BatchEmbeddingRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞—Ç—á–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    if model is None or tokenizer is None:
        raise HTTPException(status_code=500, detail="Model not initialized")
    
    if not request.texts:
        return EmbeddingResponse(
            embeddings=[],
            processing_time_ms=0.0,
            device_used=device,
            total_tokens=0,
            model_info={"model": config.MODEL, "dimension": config.DIMENSION}
        )
    
    try:
        start_time = time.time()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏
        if request.is_query:
            prefixed_texts = []
            for text in request.texts:
                query_prefix, _ = get_query_prefix(text)
                prefixed_texts.append(query_prefix + text)
        else:
            prefixed_texts = [config.DOCUMENT_PREFIX + text for text in request.texts]
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
        total_tokens = sum(
            len(tokenizer.encode(text, truncation=True, max_length=config.MAX_SEQ_LENGTH))
            for text in prefixed_texts
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–∞–º–∏
        embeddings = []
        batch_size = min(request.batch_size, config.BATCH_SIZE)
        
        with torch.no_grad():
            clear_device_cache()
            
            for i in range(0, len(prefixed_texts), batch_size):
                batch = prefixed_texts[i:i + batch_size]
                batch_embeddings = model.encode(
                    batch,
                    normalize_embeddings=True,
                    batch_size=batch_size,
                    convert_to_numpy=True,
                    show_progress_bar=False
                )
                embeddings.extend([emb.tolist() for emb in batch_embeddings])
        
        processing_time = (time.time() - start_time) * 1000
        
        logger.info(f"‚ö° Generated {len(embeddings)} embeddings in {processing_time:.1f}ms ({total_tokens} tokens) on {device}")
        
        return EmbeddingResponse(
            embeddings=embeddings,
            processing_time_ms=processing_time,
            device_used=device,
            total_tokens=total_tokens,
            model_info={
                "model": config.MODEL,
                "dimension": config.DIMENSION,
                "batch_size": len(request.texts),
                "is_query": request.is_query,
                "platform": f"{PLATFORM} {ARCHITECTURE}"
            }
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error generating batch embeddings: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Batch embedding generation failed: {str(e)}")

@app.get("/model-info")
async def get_model_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    expected_perf = "50-200ms on Apple Silicon M4 Pro" if IS_APPLE_SILICON else "100-500ms on Ubuntu (depends on hardware)"
    
    return {
        "model_name": config.MODEL,
        "dimension": config.DIMENSION,
        "max_seq_length": config.MAX_SEQ_LENGTH,
        "batch_size": config.BATCH_SIZE,
        "device": device,
        "platform": PLATFORM,
        "architecture": ARCHITECTURE,
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "mps_available": torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
        "port": 8003,
        "expected_performance": expected_perf
    }

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint"""
    return {
        "service": f"Universal Embedding Server ({PLATFORM})",
        "status": "running",
        "port": 8003,
        "model": config.MODEL,
        "dimension": config.DIMENSION,
        "platform": f"{PLATFORM} {ARCHITECTURE}",
        "endpoints": {
            "health": "/health",
            "embed": "/embed",
            "embed_batch": "/embed-batch",
            "model_info": "/model-info"
        }
    }

if __name__ == "__main__":
    print(f"üöÄ Starting Universal Embedding Server on {PLATFORM}...")
    print("üìç Port: 8003")
    print(f"ü§ñ Model: {config.MODEL}")
    print(f"üìè Dimension: {config.DIMENSION}")
    print(f"üéØ Optimized for {PLATFORM} {ARCHITECTURE}")
    
    # –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
    uvicorn.run(
        "local_embedding_server:app",
        host="0.0.0.0",  # –°–ª—É—à–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞—Ö
        port=8003,
        log_level="info",
        reload=False
    )
