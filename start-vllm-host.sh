#!/bin/bash

# ðŸš€ Ð—ÐÐŸÐ£Ð¡Ðš vLLM ÐÐ Ð¥ÐžÐ¡Ð¢Ð• Ð”Ð›Ð¯ gpt-oss-20b
# Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° vLLM ÑÐµÑ€Ð²ÐµÑ€Ð° Ð½Ð° Ubuntu Ñ GPU

set -e

# Ð¦Ð²ÐµÑ‚Ð° Ð´Ð»Ñ Ð²Ñ‹Ð²Ð¾Ð´Ð°
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}ðŸš€ Starting vLLM Host Service for gpt-oss-20b${NC}"
echo "=============================================="

# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
MODEL_NAME="openai/gpt-oss-20b"
HOST="0.0.0.0"
PORT="8000"
DTYPE="bfloat16"
MAX_MODEL_LEN="8192"
GPU_MEMORY_UTILIZATION="0.9"
TENSOR_PARALLEL_SIZE="1"

# ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL_NAME="$2"
            shift 2
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        --host)
            HOST="$2"
            shift 2
            ;;
        --dtype)
            DTYPE="$2"
            shift 2
            ;;
        --max-len)
            MAX_MODEL_LEN="$2"
            shift 2
            ;;
        --gpu-memory)
            GPU_MEMORY_UTILIZATION="$2"
            shift 2
            ;;
        --tensor-parallel)
            TENSOR_PARALLEL_SIZE="$2"
            shift 2
            ;;
        --help|-h)
            echo -e "${BLUE}Usage: $0 [OPTIONS]${NC}"
            echo ""
            echo -e "${YELLOW}Options:${NC}"
            echo "  --model MODEL           Model name (default: openai/gpt-oss-20b)"
            echo "  --port PORT             Port to bind (default: 8000)"
            echo "  --host HOST             Host to bind (default: 0.0.0.0)"
            echo "  --dtype DTYPE           Data type (default: bfloat16)"
            echo "  --max-len LENGTH        Max model length (default: 8192)"
            echo "  --gpu-memory RATIO      GPU memory utilization (default: 0.9)"
            echo "  --tensor-parallel SIZE  Tensor parallel size (default: 1)"
            echo "  --help, -h              Show this help"
            exit 0
            ;;
        *)
            echo -e "${RED}âŒ Unknown option: $1${NC}"
            exit 1
            ;;
    esac
done

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹
echo -e "${BLUE}ðŸ” Checking system requirements...${NC}"

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° NVIDIA GPU
if ! command -v nvidia-smi &> /dev/null; then
    echo -e "${RED}âŒ nvidia-smi not found. GPU required for vLLM.${NC}"
    exit 1
fi

GPU_INFO=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits 2>/dev/null || echo "")
if [ -z "$GPU_INFO" ]; then
    echo -e "${RED}âŒ No GPU detected${NC}"
    exit 1
fi

echo -e "${GREEN}âœ… GPU detected: $GPU_INFO${NC}"

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Python Ð¸ vLLM
if ! command -v python3 &> /dev/null; then
    echo -e "${RED}âŒ Python 3 not found${NC}"
    exit 1
fi

if ! python3 -c "import vllm" 2>/dev/null; then
    echo -e "${YELLOW}âš ï¸  vLLM not installed. Installing...${NC}"
    pip install vllm
fi

echo -e "${GREEN}âœ… vLLM is available${NC}"

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð¾Ñ€Ñ‚Ð°
if command -v lsof >/dev/null 2>&1; then
    if lsof -Pi :$PORT -sTCP:LISTEN -t >/dev/null 2>&1; then
        echo -e "${RED}âŒ Port $PORT is already in use${NC}"
        echo "Stop the existing service or use a different port"
        exit 1
    fi
fi

echo -e "${GREEN}âœ… Port $PORT is available${NC}"

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ Ð´Ð»Ñ Ð»Ð¾Ð³Ð¾Ð²
mkdir -p logs

# ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ
echo ""
echo -e "${BLUE}ðŸ“‹ vLLM Configuration:${NC}"
echo "  Model: $MODEL_NAME"
echo "  Host: $HOST"
echo "  Port: $PORT"
echo "  Data Type: $DTYPE"
echo "  Max Model Length: $MAX_MODEL_LEN"
echo "  GPU Memory Utilization: $GPU_MEMORY_UTILIZATION"
echo "  Tensor Parallel Size: $TENSOR_PARALLEL_SIZE"
echo ""

# Ð—Ð°Ð¿ÑƒÑÐº vLLM
echo -e "${BLUE}ðŸš€ Starting vLLM server...${NC}"

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð´Ð»Ñ HuggingFace ÐºÑÑˆÐ°
export HF_HOME="/opt/llm-cache"
export TRANSFORMERS_CACHE="/opt/llm-cache"
export HF_HUB_CACHE="/opt/llm-cache/hub"

# ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° Ð·Ð°Ð¿ÑƒÑÐºÐ° Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¼ Ð¿ÑƒÑ‚ÐµÐ¼ Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸
VLLM_CMD="vllm serve /opt/llm-cache/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee \
    --host $HOST \
    --port $PORT \
    --dtype $DTYPE \
    --max-model-len 32768 \
    --gpu-memory-utilization $GPU_MEMORY_UTILIZATION \
    --tensor-parallel-size $TENSOR_PARALLEL_SIZE \
    --disable-log-requests \
    --trust-remote-code"

echo -e "${YELLOW}Command: $VLLM_CMD${NC}"
echo ""

# Ð—Ð°Ð¿ÑƒÑÐº Ð² Ñ„Ð¾Ð½Ðµ Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼
LOG_FILE="logs/vllm_service.log"
PID_FILE="logs/vllm_service.pid"

echo -e "${BLUE}ðŸ“ Logs will be written to: $LOG_FILE${NC}"
echo -e "${BLUE}ðŸ†” PID will be saved to: $PID_FILE${NC}"
echo ""

# Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ vLLM
nohup $VLLM_CMD > "$LOG_FILE" 2>&1 &
VLLM_PID=$!

# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ PID
echo $VLLM_PID > "$PID_FILE"

echo -e "${GREEN}âœ… vLLM started with PID: $VLLM_PID${NC}"
echo ""

# Ð–Ð´ÐµÐ¼ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸
echo -e "${BLUE}â³ Waiting for vLLM to initialize...${NC}"
echo "This may take several minutes for model loading..."

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚ÑŒ ÑÐµÑ€Ð²Ð¸ÑÐ°
MAX_ATTEMPTS=60  # 5 Ð¼Ð¸Ð½ÑƒÑ‚
ATTEMPT=1

while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
    if curl -s "http://$HOST:$PORT/v1/models" > /dev/null 2>&1; then
        echo -e "${GREEN}âœ… vLLM is ready and responding!${NC}"
        break
    fi
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ ÐµÑ‰Ðµ Ð¶Ð¸Ð²
    if ! kill -0 $VLLM_PID 2>/dev/null; then
        echo -e "${RED}âŒ vLLM process died during startup${NC}"
        echo -e "${BLUE}ðŸ“‹ Check logs: tail -f $LOG_FILE${NC}"
        exit 1
    fi
    
    echo -n "."
    sleep 5
    ATTEMPT=$((ATTEMPT + 1))
done

if [ $ATTEMPT -gt $MAX_ATTEMPTS ]; then
    echo -e "${RED}âŒ vLLM failed to start within 5 minutes${NC}"
    echo -e "${BLUE}ðŸ“‹ Check logs: tail -f $LOG_FILE${NC}"
    exit 1
fi

echo ""
echo -e "${GREEN}ðŸŽ‰ vLLM is successfully running!${NC}"
echo ""

# ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÑÐµÑ€Ð²Ð¸ÑÐµ
echo -e "${BLUE}ðŸ“‹ Service Information:${NC}"
echo "  â€¢ URL: http://$HOST:$PORT"
echo "  â€¢ Models endpoint: http://$HOST:$PORT/v1/models"
echo "  â€¢ Chat endpoint: http://$HOST:$PORT/v1/chat/completions"
echo "  â€¢ PID: $VLLM_PID"
echo "  â€¢ Log file: $LOG_FILE"
echo ""

# ÐŸÐ¾Ð»ÐµÐ·Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹
echo -e "${BLUE}ðŸ› ï¸  Useful Commands:${NC}"
echo "  â€¢ View logs: tail -f $LOG_FILE"
echo "  â€¢ Check status: curl http://$HOST:$PORT/v1/models"
echo "  â€¢ Stop service: kill $VLLM_PID"
echo "  â€¢ Monitor GPU: watch -n 1 nvidia-smi"
echo ""

# Ð¢ÐµÑÑ‚ API
echo -e "${BLUE}ðŸ§ª Testing API...${NC}"
if curl -s "http://$HOST:$PORT/v1/models" | grep -q "gpt-oss"; then
    echo -e "${GREEN}âœ… API test passed - model is loaded${NC}"
else
    echo -e "${YELLOW}âš ï¸  API test inconclusive - check manually${NC}"
fi

echo ""
echo -e "${GREEN}ðŸš€ vLLM Host Service is ready for production!${NC}"
echo -e "${BLUE}ðŸ’¡ You can now start your Docker services${NC}"
echo ""

# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ð·Ð°Ð¿ÑƒÑÐºÐµ
cat > logs/vllm_info.txt << EOF
MODEL_NAME=$MODEL_NAME
HOST=$HOST
PORT=$PORT
DTYPE=$DTYPE
MAX_MODEL_LEN=$MAX_MODEL_LEN
GPU_MEMORY_UTILIZATION=$GPU_MEMORY_UTILIZATION
TENSOR_PARALLEL_SIZE=$TENSOR_PARALLEL_SIZE
PID=$VLLM_PID
STARTED_AT=$(date)
LOG_FILE=$LOG_FILE
EOF

echo -e "${BLUE}ðŸ“„ Service info saved to logs/vllm_info.txt${NC}"
