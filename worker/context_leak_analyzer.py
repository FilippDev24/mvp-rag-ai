#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É—Ç–µ—á–∫–∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç LLM
–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω–æ–µ –º–µ—Å—Ç–æ, –≥–¥–µ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –ø—Ä–æ—Ö–æ–¥—è—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
"""

import os
import sys
import json
import time
import logging
from typing import List, Dict, Any, Optional, Set
import re
import numpy as np

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ worker –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.database_service import DatabaseService
from services.embedding_service import EmbeddingService
from services.search_service import get_search_service
from services.local_reranking_service import LocalRerankingService
from services.reranking_service import RerankingService

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ContextLeakAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É—Ç–µ—á–∫–∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç LLM
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.database_service = None
        self.embedding_service = None
        self.search_service = None
        self.reranking_service = None
        self._initialize_services()
    
    def _initialize_services(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
        try:
            logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤...")
            
            self.database_service = DatabaseService()
            self.embedding_service = EmbeddingService()
            
            try:
                self.reranking_service = LocalRerankingService()
                logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º LocalRerankingService")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  LocalRerankingService –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.reranking_service = RerankingService()
                logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º RerankingService")
            
            self.search_service = get_search_service(
                self.database_service,
                self.embedding_service, 
                self.reranking_service
            )
            
            logger.info("‚úÖ –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Ä–≤–∏—Å–æ–≤: {e}")
            raise
    
    def analyze_context_leak(
        self, 
        query: str, 
        access_level: int = 100,
        expected_keywords: List[str] = None
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É—Ç–µ—á–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            access_level: –£—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç—É–ø–∞
            expected_keywords: –û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
            
        Returns:
            –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É—Ç–µ—á–∫–∏
        """
        logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ —É—Ç–µ—á–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
        
        if expected_keywords is None:
            expected_keywords = query.lower().split()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º BM25 –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        self.search_service._ensure_bm25_initialized(access_level)
        
        # 1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        logger.info("üìä –≠—Ç–∞–ø 1: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫...")
        vector_results, embedding_metrics = self.search_service._vector_search(query, access_level, 30)
        vector_analysis = self._analyze_relevance(vector_results, expected_keywords, "–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫")
        
        # 2. BM25 –ø–æ–∏—Å–∫
        logger.info("üìù –≠—Ç–∞–ø 2: BM25 –ø–æ–∏—Å–∫...")
        bm25_results = self.search_service._bm25_search(query, access_level, 30)
        bm25_analysis = self._analyze_relevance(bm25_results, expected_keywords, "BM25 –ø–æ–∏—Å–∫")
        
        # 3. RRF Fusion
        logger.info("üîÄ –≠—Ç–∞–ø 3: RRF fusion...")
        fused_results = self.search_service._rrf_fusion(vector_results, bm25_results)
        fusion_analysis = self._analyze_relevance(fused_results, expected_keywords, "RRF Fusion")
        
        # 4. –†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ë–ï–ó —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        logger.info("üéØ –≠—Ç–∞–ø 4: –†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (–±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)...")
        documents = [result["content"] for result in fused_results]
        raw_reranked = self.reranking_service.rerank_results(query, documents, len(documents))
        
        # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        all_reranked_results = []
        for rerank_result in raw_reranked:
            original_index = rerank_result["index"]
            if original_index < len(fused_results):
                result = fused_results[original_index].copy()
                result.update({
                    "rerank_score": rerank_result["score"],
                    "raw_logit": rerank_result.get("raw_logit", 0),
                })
                all_reranked_results.append(result)
        
        rerank_analysis = self._analyze_relevance(all_reranked_results, expected_keywords, "–†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (–≤—Å–µ)")
        
        # 5. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–∫–∞–∫ –≤ search_service)
        logger.info("üö™ –≠—Ç–∞–ø 5: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏...")
        scores = [r["rerank_score"] for r in all_reranked_results]
        
        if scores:
            best_score = max(scores)
            worst_score = min(scores)
            score_range = best_score - worst_score
            
            # –ö–æ–ø–∏—Ä—É–µ–º –ª–æ–≥–∏–∫—É –∏–∑ search_service
            if score_range > 2.0:
                high_threshold = best_score * 0.8
                general_threshold = best_score * 0.4
            elif score_range > 1.0:
                high_threshold = best_score * 0.7
                general_threshold = best_score * 0.3
            else:
                high_threshold = best_score - 0.1
                general_threshold = best_score * 0.5
        else:
            high_threshold = 0
            general_threshold = 0
            best_score = 0
            worst_score = 0
            score_range = 0
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥–∞–º
        filtered_results = [r for r in all_reranked_results if r["rerank_score"] >= high_threshold]
        filter_analysis = self._analyze_relevance(filtered_results, expected_keywords, "–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        
        # 6. –ê–Ω–∞–ª–∏–∑ —É—Ç–µ—á–∫–∏
        leak_analysis = self._detect_leaks(
            query, expected_keywords, vector_analysis, bm25_analysis, 
            fusion_analysis, rerank_analysis, filter_analysis,
            {
                'best_score': best_score,
                'worst_score': worst_score,
                'score_range': score_range,
                'high_threshold': high_threshold,
                'general_threshold': general_threshold
            }
        )
        
        return {
            'query': query,
            'expected_keywords': expected_keywords,
            'stages': {
                'vector': vector_analysis,
                'bm25': bm25_analysis,
                'fusion': fusion_analysis,
                'rerank_all': rerank_analysis,
                'filtered': filter_analysis
            },
            'thresholds': {
                'best_score': best_score,
                'worst_score': worst_score,
                'score_range': score_range,
                'high_threshold': high_threshold,
                'general_threshold': general_threshold
            },
            'leak_analysis': leak_analysis,
            'final_context_results': filtered_results
        }
    
    def _analyze_relevance(
        self, 
        results: List[Dict], 
        expected_keywords: List[str], 
        stage_name: str
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ"""
        
        if not results:
            return {
                'stage': stage_name,
                'total_results': 0,
                'relevant_results': 0,
                'irrelevant_results': 0,
                'relevance_ratio': 0.0,
                'results_details': []
            }
        
        relevant_count = 0
        irrelevant_count = 0
        results_details = []
        
        for i, result in enumerate(results):
            content = result.get('content', '').lower()
            metadata = result.get('metadata', {})
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–∂–∏–¥–∞–µ–º—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_matches = []
            for keyword in expected_keywords:
                if keyword.lower() in content:
                    keyword_matches.append(keyword)
            
            is_relevant = len(keyword_matches) > 0
            
            if is_relevant:
                relevant_count += 1
            else:
                irrelevant_count += 1
            
            result_detail = {
                'rank': i + 1,
                'id': result.get('id', 'unknown'),
                'document_title': metadata.get('doc_title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                'content_preview': content[:200] + '...' if len(content) > 200 else content,
                'is_relevant': is_relevant,
                'keyword_matches': keyword_matches,
                'score': result.get('score', 0),
                'rerank_score': result.get('rerank_score', 0),
                'raw_logit': result.get('raw_logit', 0)
            }
            results_details.append(result_detail)
        
        return {
            'stage': stage_name,
            'total_results': len(results),
            'relevant_results': relevant_count,
            'irrelevant_results': irrelevant_count,
            'relevance_ratio': relevant_count / len(results) if results else 0.0,
            'results_details': results_details
        }
    
    def _detect_leaks(
        self, 
        query: str,
        expected_keywords: List[str],
        vector_analysis: Dict,
        bm25_analysis: Dict,
        fusion_analysis: Dict,
        rerank_analysis: Dict,
        filter_analysis: Dict,
        thresholds: Dict
    ) -> Dict[str, Any]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ç–µ—á–µ–∫ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        leaks = []
        warnings = []
        critical_issues = []
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Ç–µ—á–∫—É –Ω–∞ —ç—Ç–∞–ø–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        if vector_analysis['irrelevant_results'] > vector_analysis['relevant_results']:
            leaks.append({
                'stage': 'vector_search',
                'issue': '–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–æ–ª—å—à–µ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤',
                'irrelevant_count': vector_analysis['irrelevant_results'],
                'relevant_count': vector_analysis['relevant_results'],
                'severity': 'high'
            })
        
        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Ç–µ—á–∫—É –Ω–∞ —ç—Ç–∞–ø–µ BM25
        if bm25_analysis['irrelevant_results'] > bm25_analysis['relevant_results']:
            leaks.append({
                'stage': 'bm25_search',
                'issue': 'BM25 –ø–æ–∏—Å–∫ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–æ–ª—å—à–µ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤',
                'irrelevant_count': bm25_analysis['irrelevant_results'],
                'relevant_count': bm25_analysis['relevant_results'],
                'severity': 'high'
            })
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Ç–µ—á–∫—É –ø–æ—Å–ª–µ fusion
        if fusion_analysis['irrelevant_results'] > fusion_analysis['relevant_results']:
            leaks.append({
                'stage': 'rrf_fusion',
                'issue': 'RRF Fusion –Ω–µ —É–ª—É—á—à–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å',
                'irrelevant_count': fusion_analysis['irrelevant_results'],
                'relevant_count': fusion_analysis['relevant_results'],
                'severity': 'medium'
            })
        
        # 4. –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Ç–µ—á–∫—É –ø–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        if rerank_analysis['irrelevant_results'] > 0:
            # –ù–∞—Ö–æ–¥–∏–º –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤—ã—Å–æ–∫–∏–º–∏ —Å–∫–æ—Ä–∞–º–∏
            high_score_irrelevant = [
                r for r in rerank_analysis['results_details'] 
                if not r['is_relevant'] and r['rerank_score'] > 5.0
            ]
            
            if high_score_irrelevant:
                critical_issues.append({
                    'stage': 'reranking',
                    'issue': '–†–µ—Ä–∞–Ω–∂–µ—Ä –¥–∞–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Å–∫–æ—Ä—ã –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º',
                    'high_score_irrelevant': high_score_irrelevant,
                    'severity': 'critical'
                })
        
        # 5. –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —É—Ç–µ—á–∫—É –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if filter_analysis['irrelevant_results'] > 0:
            final_irrelevant = [
                r for r in filter_analysis['results_details'] 
                if not r['is_relevant']
            ]
            
            critical_issues.append({
                'stage': 'final_context',
                'issue': '–£–¢–ï–ß–ö–ê: –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ø–∞–¥–∞—é—Ç –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç LLM',
                'leaked_results': final_irrelevant,
                'severity': 'critical'
            })
        
        # 6. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        if thresholds['score_range'] < 1.0:
            warnings.append({
                'stage': 'threshold_calculation',
                'issue': '–ú–∞–ª—ã–π —Ä–∞–∑–±—Ä–æ—Å —Å–∫–æ—Ä–æ–≤ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏',
                'score_range': thresholds['score_range'],
                'severity': 'medium'
            })
        
        # 7. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ—Ä–æ–≥–æ–≤
        if thresholds['high_threshold'] < 3.0:
            warnings.append({
                'stage': 'adaptive_thresholds',
                'issue': '–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏',
                'high_threshold': thresholds['high_threshold'],
                'severity': 'medium'
            })
        
        return {
            'total_leaks': len(leaks),
            'total_warnings': len(warnings),
            'total_critical': len(critical_issues),
            'leaks': leaks,
            'warnings': warnings,
            'critical_issues': critical_issues,
            'leak_summary': self._generate_leak_summary(leaks, warnings, critical_issues)
        }
    
    def _generate_leak_summary(
        self, 
        leaks: List[Dict], 
        warnings: List[Dict], 
        critical_issues: List[Dict]
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ —É—Ç–µ—á–µ–∫"""
        
        if critical_issues:
            return f"üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –£–¢–ï–ß–ö–ê: {len(critical_issues)} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"
        elif leaks:
            return f"üü° –£–¢–ï–ß–ö–ê –û–ë–ù–ê–†–£–ñ–ï–ù–ê: {len(leaks)} –ø—Ä–æ–±–ª–µ–º —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é"
        elif warnings:
            return f"üü° –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø: {len(warnings)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º"
        else:
            return "‚úÖ –£–¢–ï–ß–ï–ö –ù–ï –û–ë–ù–ê–†–£–ñ–ï–ù–û: –°–∏—Å—Ç–µ–º–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ"
    
    def print_leak_report(self, analysis: Dict[str, Any]):
        """–ü–µ—á–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ–± —É—Ç–µ—á–∫–∞—Ö"""
        
        print("\n" + "="*80)
        print("üîç –û–¢–ß–ï–¢ –û–ë –£–¢–ï–ß–ö–ï –ù–ï–†–ï–õ–ï–í–ê–ù–¢–ù–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("="*80)
        print(f"–ó–∞–ø—Ä–æ—Å: '{analysis['query']}'")
        print(f"–û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {analysis['expected_keywords']}")
        print()
        
        # –†–µ–∑—é–º–µ –ø–æ —ç—Ç–∞–ø–∞–º
        print("üìä –ê–ù–ê–õ–ò–ó –ü–û –≠–¢–ê–ü–ê–ú:")
        for stage_name, stage_data in analysis['stages'].items():
            relevance_pct = stage_data['relevance_ratio'] * 100
            print(f"  {stage_data['stage']}: {stage_data['relevant_results']}/{stage_data['total_results']} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö ({relevance_pct:.1f}%)")
        print()
        
        # –ü–æ—Ä–æ–≥–∏
        thresholds = analysis['thresholds']
        print("üö™ –ü–û–†–û–ì–ò –§–ò–õ–¨–¢–†–ê–¶–ò–ò:")
        print(f"  –õ—É—á—à–∏–π —Å–∫–æ—Ä: {thresholds['best_score']:.3f}")
        print(f"  –•—É–¥—à–∏–π —Å–∫–æ—Ä: {thresholds['worst_score']:.3f}")
        print(f"  –†–∞–∑–±—Ä–æ—Å: {thresholds['score_range']:.3f}")
        print(f"  –ü–æ—Ä–æ–≥ –≤—ã—Å–æ–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {thresholds['high_threshold']:.3f}")
        print(f"  –ü–æ—Ä–æ–≥ –æ–±—â–µ–≥–æ —á–∞—Ç–∞: {thresholds['general_threshold']:.3f}")
        print()
        
        # –ê–Ω–∞–ª–∏–∑ —É—Ç–µ—á–µ–∫
        leak_analysis = analysis['leak_analysis']
        print("üö® –ê–ù–ê–õ–ò–ó –£–¢–ï–ß–ï–ö:")
        print(f"  {leak_analysis['leak_summary']}")
        print(f"  –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º: {leak_analysis['total_critical']}")
        print(f"  –£—Ç–µ—á–µ–∫: {leak_analysis['total_leaks']}")
        print(f"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {leak_analysis['total_warnings']}")
        print()
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        if leak_analysis['critical_issues']:
            print("üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:")
            for issue in leak_analysis['critical_issues']:
                print(f"  ‚Ä¢ {issue['issue']} (—ç—Ç–∞–ø: {issue['stage']})")
                if 'leaked_results' in issue:
                    for leaked in issue['leaked_results'][:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                        print(f"    - –†–∞–Ω–≥ #{leaked['rank']}: {leaked['content_preview'][:100]}...")
            print()
        
        # –£—Ç–µ—á–∫–∏
        if leak_analysis['leaks']:
            print("üü° –û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –£–¢–ï–ß–ö–ò:")
            for leak in leak_analysis['leaks']:
                print(f"  ‚Ä¢ {leak['issue']} (—ç—Ç–∞–ø: {leak['stage']})")
                print(f"    –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {leak['irrelevant_count']}, –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {leak['relevant_count']}")
            print()
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        if leak_analysis['warnings']:
            print("‚ö†Ô∏è  –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:")
            for warning in leak_analysis['warnings']:
                print(f"  ‚Ä¢ {warning['issue']} (—ç—Ç–∞–ø: {warning['stage']})")
            print()
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø–æ–ø–∞–¥–∞—é—â–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        final_results = analysis['final_context_results']
        if final_results:
            print("üìã –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í –ö–û–ù–¢–ï–ö–°–¢–ï LLM:")
            for i, result in enumerate(final_results[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                metadata = result.get('metadata', {})
                relevance = "‚úÖ –†–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω" if any(kw.lower() in result.get('content', '').lower() for kw in analysis['expected_keywords']) else "‚ùå –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω"
                print(f"  #{i+1} (—Å–∫–æ—Ä: {result.get('rerank_score', 0):.3f}) {relevance}")
                print(f"      –î–æ–∫—É–º–µ–Ω—Ç: {metadata.get('doc_title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                content_preview = result.get('content', '')[:150] + '...' if len(result.get('content', '')) > 150 else result.get('content', '')
                print(f"      –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {content_preview}")
                print()

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —É—Ç–µ—á–µ–∫"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ —É—Ç–µ—á–∫–∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç LLM')
    parser.add_argument('--query', '-q', type=str, default="–ü—Ä–∏—à–ª–∏ –ø–æ—á—Ç—É –ê–Ω—Ç–æ–Ω–∞", help='–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('--access-level', '-a', type=int, default=100, help='–£—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç—É–ø–∞')
    parser.add_argument('--keywords', '-k', type=str, nargs='+', help='–û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª)')
    parser.add_argument('--json', '-j', action='store_true', help='–í—ã–≤–æ–¥ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ')
    
    args = parser.parse_args()
    
    try:
        analyzer = ContextLeakAnalyzer()
        
        expected_keywords = args.keywords if args.keywords else None
        
        analysis = analyzer.analyze_context_leak(
            args.query, 
            args.access_level, 
            expected_keywords
        )
        
        if args.json:
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π JSON –≤—ã–≤–æ–¥
            output = {
                'query': analysis['query'],
                'leak_summary': analysis['leak_analysis']['leak_summary'],
                'critical_issues_count': analysis['leak_analysis']['total_critical'],
                'leaks_count': analysis['leak_analysis']['total_leaks'],
                'final_results_count': len(analysis['final_context_results']),
                'relevance_by_stage': {
                    stage: data['relevance_ratio'] 
                    for stage, data in analysis['stages'].items()
                }
            }
            print(json.dumps(output, ensure_ascii=False, indent=2))
        else:
            analyzer.print_leak_report(analysis)
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —É—Ç–µ—á–µ–∫: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
