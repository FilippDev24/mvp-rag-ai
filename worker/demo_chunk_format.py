#!/usr/bin/env python3
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —á–∞–Ω–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –ò–ò
"""

import os
import sys
import json
from typing import Dict, Any, List

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ worker –≤ sys.path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from services.document_analyzer import DocumentStructureAnalyzer
from services.chunking_service import SemanticChunkingService
from processors.docx_processor import DocxProcessor

def demonstrate_chunk_format():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ç–æ–≥–æ, –∫–∞–∫ —á–∞–Ω–∫–∏ –≤—ã–≥–ª—è–¥—è—Ç –¥–ª—è –ò–ò"""
    
    # –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É
    test_file = "../–ü—Ä–∏–∫–∞–∑_–æ–±_–∏–∑–º–µ–Ω–µ–Ω–∏–∏_–¥–æ–ª–∂–Ω–æ—Å—Ç–Ω–æ–π_–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏_–∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä–∞.docx"
    
    if not os.path.exists(test_file):
        test_file = "test_document.docx"
    
    if not os.path.exists(test_file):
        print("‚ùå –¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    print("üîç –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø: –ö–∞–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ–ø–∞–¥–∞–µ—Ç –≤ –ò–ò")
    print("=" * 60)
    
    try:
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        processor = DocxProcessor()
        result = processor.process_document(test_file, "demo_doc", 50)
        
        if not result["success"]:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result['error']}")
            return
        
        # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π chunking
        chunking_service = SemanticChunkingService()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–µ–∫—Ü–∏–∏ –≤ –æ–±—ä–µ–∫—Ç—ã
        from services.document_analyzer import DocumentSection
        sections_objects = []
        for section_data in result["document_sections"]:
            section = DocumentSection(
                title=section_data["title"],
                content=section_data["content"],
                level=section_data["level"],
                section_type=section_data["section_type"],
                start_pos=section_data["start_pos"],
                end_pos=section_data["end_pos"],
                metadata=section_data["metadata"]
            )
            sections_objects.append(section)
        
        chunks_data = chunking_service.create_chunks(
            result["text"],
            "demo_doc",
            50,
            document_sections=sections_objects,
            document_metadata=result["document_metadata"]
        )
        
        print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(chunks_data)} —á–∞–Ω–∫–æ–≤")
        print()
        
        # 3. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –ø–æ–ø–∞–¥–∞–µ—Ç –≤ –ò–ò
        print("ü§ñ –ß–¢–û –í–ò–î–ò–¢ –ò–ò –ü–†–ò –ü–û–ò–°–ö–ï:")
        print("=" * 60)
        
        for i, chunk in enumerate(chunks_data[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 —á–∞–Ω–∫–∞
            print(f"\nüìù –ß–ê–ù–ö {i+1}:")
            print("-" * 40)
            
            # –¢–µ–∫—Å—Ç —á–∞–Ω–∫–∞ (–æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ)
            print("üî§ –¢–ï–ö–°–¢ –ß–ê–ù–ö–ê (—Ç–æ, —á—Ç–æ –∏—â–µ—Ç—Å—è):")
            print(f'"{chunk["text"]}"')
            print()
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ò–ò)
            metadata = chunk["metadata"]
            print("üìã –ú–ï–¢–ê–î–ê–ù–ù–´–ï (–∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ò–ò):")
            
            # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            print(f"  üìÇ –°–µ–∫—Ü–∏—è: {metadata.get('section_title', 'N/A')}")
            print(f"  üè∑Ô∏è  –¢–∏–ø —Å–µ–∫—Ü–∏–∏: {metadata.get('section_type', 'N/A')}")
            print(f"  üìä –£—Ä–æ–≤–µ–Ω—å: {metadata.get('section_level', 'N/A')}")
            print(f"  üß© –¢–∏–ø —á–∞–Ω–∫–∞: {metadata.get('chunk_type', 'N/A')}")
            print(f"  ‚úÖ –ü–æ–ª–Ω–∞—è —Å–µ–∫—Ü–∏—è: {metadata.get('is_complete_section', False)}")
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            print(f"  üìÑ –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {metadata.get('document_type', 'N/A')}")
            print(f"  üìã –ù–∞–∑–≤–∞–Ω–∏–µ: {metadata.get('document_title', 'N/A')}")
            print(f"  üî¢ –ù–æ–º–µ—Ä: {metadata.get('document_number', 'N/A')}")
            print(f"  üìÖ –î–∞—Ç–∞: {metadata.get('document_date', 'N/A')}")
            print(f"  üè¢ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: {metadata.get('document_organization', 'N/A')}")
            
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            print(f"  üìè –†–∞–∑–º–µ—Ä: {metadata.get('char_count', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"  üîê –£—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç—É–ø–∞: {metadata.get('access_level', 'N/A')}")
            print(f"  üìç –ü–æ–∑–∏—Ü–∏—è: {metadata.get('chunk_index', 0)}/{metadata.get('total_chunks', 0)}")
            
            print()
        
        # 4. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ò–ò
        print("\nüéØ –ö–û–ù–¢–ï–ö–°–¢ –î–õ–Ø –ò–ò –ü–†–ò –û–¢–í–ï–¢–ï:")
        print("=" * 60)
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º, –∫–∞–∫ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ò–ò
        context_parts = []
        for i, chunk in enumerate(chunks_data[:2]):
            metadata = chunk["metadata"]
            doc_title = metadata.get("document_title", "–î–æ–∫—É–º–µ–Ω—Ç")
            section_title = metadata.get("section_title", "")
            
            # –§–æ—Ä–º–∞—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤–∏–¥–∏—Ç –ò–ò
            if section_title and section_title != doc_title:
                context_part = f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i+1}: {doc_title} - {section_title}]\n{chunk['text']}"
            else:
                context_part = f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i+1}: {doc_title}]\n{chunk['text']}"
            
            context_parts.append(context_part)
        
        full_context = "\n\n".join(context_parts)
        
        print("üìù –ò–¢–û–ì–û–í–´–ô –ö–û–ù–¢–ï–ö–°–¢:")
        print(f'"""\n{full_context}\n"""')
        
        # 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:")
        print("=" * 60)
        stats = chunking_service.get_chunking_stats(chunks_data)
        
        print(f"üìÑ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {stats['total_chunks']}")
        print(f"üìè –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {stats['avg_chunk_size']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üìê –†–∞–∑–º–µ—Ä –æ—Ç {stats['min_chunk_size']} –¥–æ {stats['max_chunk_size']} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üìä –¢–∏–ø—ã —á–∞–Ω–∫–æ–≤: {stats['chunk_types']}")
        print(f"üè∑Ô∏è  –¢–∏–ø—ã —Å–µ–∫—Ü–∏–π: {stats['section_types']}")
        print(f"‚úÖ –ü–æ–ª–Ω—ã—Ö —Å–µ–∫—Ü–∏–π: {stats['complete_sections']}")
        print(f"üß© –ß–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–µ–∫—Ü–∏–π: {stats['partial_sections']}")
        
        # 6. –ü—Ä–∏–º–µ—Ä JSON –¥–ª—è API
        print(f"\nüîå –ü–†–ò–ú–ï–† JSON –î–õ–Ø API:")
        print("=" * 60)
        
        example_chunk = chunks_data[0]
        api_format = {
            "chunk_id": f"{example_chunk['metadata']['doc_id']}_{example_chunk['metadata']['chunk_index']}",
            "text": example_chunk["text"][:200] + "..." if len(example_chunk["text"]) > 200 else example_chunk["text"],
            "metadata": {
                "document_type": example_chunk["metadata"].get("document_type"),
                "document_title": example_chunk["metadata"].get("document_title"),
                "section_title": example_chunk["metadata"].get("section_title"),
                "section_type": example_chunk["metadata"].get("section_type"),
                "chunk_type": example_chunk["metadata"].get("chunk_type"),
                "access_level": example_chunk["metadata"].get("access_level"),
                "similarity_score": 0.85,  # –ü—Ä–∏–º–µ—Ä —Å–∫–æ—Ä–∞
                "rerank_score": 0.92       # –ü—Ä–∏–º–µ—Ä —Å–∫–æ—Ä–∞
            }
        }
        
        print(json.dumps(api_format, ensure_ascii=False, indent=2))
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    demonstrate_chunk_format()
