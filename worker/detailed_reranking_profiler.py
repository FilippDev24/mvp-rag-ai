"""
–î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤—â–∏–∫ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω—ã –∑–∞–º–µ–¥–ª–µ–Ω–∏—è –≤ pipeline
"""

import time
import psutil
import gc
import torch
import threading
import logging
from typing import List, Dict, Any
from services.reranking_service import RerankingService
from services.search_service import get_search_service
from services.database_service import DatabaseService
from services.embedding_service import EmbeddingService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DetailedRerankingProfiler:
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self):
        self.process = psutil.Process()
        
    def get_memory_info(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏"""
        memory_info = self.process.memory_info()
        return {
            "rss_mb": memory_info.rss / 1024 / 1024,
            "vms_mb": memory_info.vms / 1024 / 1024,
            "percent": self.process.memory_percent()
        }
    
    def get_gpu_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU"""
        if torch.cuda.is_available():
            return {
                "available": True,
                "device_count": torch.cuda.device_count(),
                "current_device": torch.cuda.current_device(),
                "memory_allocated": torch.cuda.memory_allocated() / 1024 / 1024,
                "memory_reserved": torch.cuda.memory_reserved() / 1024 / 1024,
                "memory_cached": torch.cuda.memory_cached() / 1024 / 1024 if hasattr(torch.cuda, 'memory_cached') else 0
            }
        return {"available": False}
    
    def profile_isolated_reranking(self, query: str, documents: List[str], iterations: int = 3) -> Dict[str, Any]:
        """–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logger.info(f"=== –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï –ò–ó–û–õ–ò–†–û–í–ê–ù–ù–û–ì–û –†–ï–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø ===")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
        reranking_service = RerankingService()
        
        results = []
        
        for i in range(iterations):
            logger.info(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}/{iterations}")
            
            # –°–±–æ—Ä –º—É—Å–æ—Ä–∞ –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–æ–º
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # –ù–∞—á–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            start_memory = self.get_memory_info()
            start_gpu = self.get_gpu_info()
            start_time = time.time()
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            reranked_results = reranking_service.rerank_results(query, documents, top_k=10)
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            end_time = time.time()
            end_memory = self.get_memory_info()
            end_gpu = self.get_gpu_info()
            
            execution_time = (end_time - start_time) * 1000
            
            iteration_result = {
                "iteration": i + 1,
                "execution_time_ms": execution_time,
                "memory_before": start_memory,
                "memory_after": end_memory,
                "memory_delta_mb": end_memory["rss_mb"] - start_memory["rss_mb"],
                "gpu_before": start_gpu,
                "gpu_after": end_gpu,
                "results_count": len(reranked_results),
                "top_score": reranked_results[0]["score"] if reranked_results else 0
            }
            
            results.append(iteration_result)
            logger.info(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.1f}ms")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        times = [r["execution_time_ms"] for r in results]
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        return {
            "type": "isolated_reranking",
            "iterations": iterations,
            "results": results,
            "statistics": {
                "avg_time_ms": avg_time,
                "min_time_ms": min_time,
                "max_time_ms": max_time,
                "std_dev_ms": (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5
            }
        }
    
    def profile_pipeline_reranking(self, query: str, access_level: int = 50, iterations: int = 3) -> Dict[str, Any]:
        """–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–æ—Å—Ç–∞–≤–µ SearchService pipeline"""
        logger.info(f"=== –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï –†–ï–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø –í PIPELINE ===")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
        database_service = DatabaseService()
        embedding_service = EmbeddingService()
        reranking_service = RerankingService()
        search_service = get_search_service(database_service, embedding_service, reranking_service)
        
        results = []
        
        for i in range(iterations):
            logger.info(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}/{iterations}")
            
            # –°–±–æ—Ä –º—É—Å–æ—Ä–∞ –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–æ–º
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # –ù–∞—á–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            start_memory = self.get_memory_info()
            start_gpu = self.get_gpu_info()
            start_time = time.time()
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            search_results = search_service.hybrid_search(
                query=query,
                access_level=access_level,
                top_k=30,
                rerank_top_k=10
            )
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            end_time = time.time()
            end_memory = self.get_memory_info()
            end_gpu = self.get_gpu_info()
            
            execution_time = (end_time - start_time) * 1000
            
            iteration_result = {
                "iteration": i + 1,
                "execution_time_ms": execution_time,
                "memory_before": start_memory,
                "memory_after": end_memory,
                "memory_delta_mb": end_memory["rss_mb"] - start_memory["rss_mb"],
                "gpu_before": start_gpu,
                "gpu_after": end_gpu,
                "search_success": search_results.get("success", False),
                "results_count": search_results.get("final_results_count", 0),
                "vector_results": search_results.get("vector_results_count", 0),
                "bm25_results": search_results.get("bm25_results_count", 0),
                "fused_results": search_results.get("fused_results_count", 0),
                "embedding_time_ms": search_results.get("embedding_time_ms", 0)
            }
            
            results.append(iteration_result)
            logger.info(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è pipeline: {execution_time:.1f}ms")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        times = [r["execution_time_ms"] for r in results]
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        return {
            "type": "pipeline_reranking",
            "iterations": iterations,
            "results": results,
            "statistics": {
                "avg_time_ms": avg_time,
                "min_time_ms": min_time,
                "max_time_ms": max_time,
                "std_dev_ms": (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5
            }
        }
    
    def profile_step_by_step_pipeline(self, query: str, access_level: int = 50) -> Dict[str, Any]:
        """–ü–æ—à–∞–≥–æ–≤–æ–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ pipeline –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —É–∑–∫–∏—Ö –º–µ—Å—Ç"""
        logger.info(f"=== –ü–û–®–ê–ì–û–í–û–ï –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï PIPELINE ===")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
        database_service = DatabaseService()
        embedding_service = EmbeddingService()
        reranking_service = RerankingService()
        search_service = get_search_service(database_service, embedding_service, reranking_service)
        
        # –°–±–æ—Ä –º—É—Å–æ—Ä–∞
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        steps_timing = {}
        memory_tracking = {}
        
        # –û–±—â–µ–µ –Ω–∞—á–∞–ª–æ
        total_start = time.time()
        start_memory = self.get_memory_info()
        
        # –®–∞–≥ 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BM25
        step_start = time.time()
        search_service._ensure_bm25_initialized(access_level)
        steps_timing["bm25_init"] = (time.time() - step_start) * 1000
        memory_tracking["after_bm25_init"] = self.get_memory_info()
        
        # –®–∞–≥ 2: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        step_start = time.time()
        vector_results, embedding_metrics = search_service._vector_search(query, access_level, 30)
        steps_timing["vector_search"] = (time.time() - step_start) * 1000
        steps_timing["embedding_time"] = embedding_metrics.get("embedding_time_ms", 0)
        memory_tracking["after_vector_search"] = self.get_memory_info()
        
        # –®–∞–≥ 3: BM25 –ø–æ–∏—Å–∫
        step_start = time.time()
        bm25_results = search_service._bm25_search(query, access_level, 30)
        steps_timing["bm25_search"] = (time.time() - step_start) * 1000
        memory_tracking["after_bm25_search"] = self.get_memory_info()
        
        # –®–∞–≥ 4: RRF Fusion
        step_start = time.time()
        fused_results = search_service._rrf_fusion(vector_results, bm25_results, 0.7, 0.3)
        steps_timing["rrf_fusion"] = (time.time() - step_start) * 1000
        memory_tracking["after_rrf_fusion"] = self.get_memory_info()
        
        # –®–∞–≥ 5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—é
        step_start = time.time()
        documents = [result["content"] for result in fused_results]
        steps_timing["rerank_preparation"] = (time.time() - step_start) * 1000
        memory_tracking["before_reranking"] = self.get_memory_info()
        
        # –®–∞–≥ 6: –î–ï–¢–ê–õ–¨–ù–û–ï –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï –†–ï–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # 6.1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä
        step_start = time.time()
        pairs = [(query, doc) for doc in documents]
        steps_timing["rerank_pairs_preparation"] = (time.time() - step_start) * 1000
        
        # 6.2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)
        step_start = time.time()
        model_loaded = reranking_service.model is not None
        if not model_loaded:
            reranking_service._load_model()
        steps_timing["rerank_model_loading"] = (time.time() - step_start) * 1000
        
        # 6.3: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–∫–æ—Ä–æ–≤
        step_start = time.time()
        memory_before_predict = self.get_memory_info()
        gpu_before_predict = self.get_gpu_info()
        
        with torch.no_grad():
            scores = reranking_service.model.predict(pairs)
        
        steps_timing["rerank_model_predict"] = (time.time() - step_start) * 1000
        memory_after_predict = self.get_memory_info()
        gpu_after_predict = self.get_gpu_info()
        
        # 6.4: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫–æ—Ä–æ–≤
        step_start = time.time()
        import numpy as np
        scores_array = np.array(scores)
        max_logit = np.max(scores_array)
        amplification_factor = 100
        amplified_scores = np.exp(scores_array * amplification_factor)
        min_amplified = np.min(amplified_scores)
        max_amplified = np.max(amplified_scores)
        
        if max_amplified > min_amplified:
            final_scores = (amplified_scores - min_amplified) / (max_amplified - min_amplified) * 10
        else:
            final_scores = np.ones_like(amplified_scores) * 5.0
        
        steps_timing["rerank_postprocessing"] = (time.time() - step_start) * 1000
        
        # 6.5: –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        step_start = time.time()
        rerank_results = []
        for i, (raw_score, final_score) in enumerate(zip(scores_array, final_scores)):
            rerank_results.append({
                "index": i,
                "score": float(final_score),
                "raw_logit": float(raw_score),
                "document": documents[i]
            })
        rerank_results.sort(key=lambda x: x["score"], reverse=True)
        top_results = rerank_results[:10]
        
        steps_timing["rerank_results_creation"] = (time.time() - step_start) * 1000
        
        # –û–±—â–µ–µ –≤—Ä–µ–º—è —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        total_rerank_time = (
            steps_timing["rerank_pairs_preparation"] +
            steps_timing["rerank_model_loading"] +
            steps_timing["rerank_model_predict"] +
            steps_timing["rerank_postprocessing"] +
            steps_timing["rerank_results_creation"]
        )
        steps_timing["total_reranking"] = total_rerank_time
        
        memory_tracking["after_reranking"] = self.get_memory_info()
        
        # –û–±—â–µ–µ –≤—Ä–µ–º—è
        total_time = (time.time() - total_start) * 1000
        steps_timing["total_pipeline"] = total_time
        
        # –ê–Ω–∞–ª–∏–∑ –ø–∞–º—è—Ç–∏
        memory_deltas = {}
        prev_memory = start_memory["rss_mb"]
        for step, memory_info in memory_tracking.items():
            current_memory = memory_info["rss_mb"]
            memory_deltas[step] = current_memory - prev_memory
            prev_memory = current_memory
        
        return {
            "type": "step_by_step_pipeline",
            "query": query,
            "access_level": access_level,
            "documents_count": len(documents),
            "steps_timing_ms": steps_timing,
            "memory_tracking": memory_tracking,
            "memory_deltas_mb": memory_deltas,
            "reranking_details": {
                "model_loaded_initially": model_loaded,
                "pairs_count": len(pairs),
                "memory_before_predict": memory_before_predict,
                "memory_after_predict": memory_after_predict,
                "memory_delta_predict_mb": memory_after_predict["rss_mb"] - memory_before_predict["rss_mb"],
                "gpu_before_predict": gpu_before_predict,
                "gpu_after_predict": gpu_after_predict,
                "scores_stats": {
                    "min_raw": float(scores_array.min()),
                    "max_raw": float(scores_array.max()),
                    "mean_raw": float(scores_array.mean()),
                    "min_final": float(final_scores.min()),
                    "max_final": float(final_scores.max()),
                    "mean_final": float(final_scores.mean())
                }
            },
            "performance_analysis": {
                "reranking_percentage": (total_rerank_time / total_time) * 100,
                "model_predict_percentage": (steps_timing["rerank_model_predict"] / total_time) * 100,
                "slowest_step": max(steps_timing.items(), key=lambda x: x[1])
            }
        }
    
    def run_comprehensive_analysis(self, query: str = "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –µ—Å—Ç—å –≤ —Å–∏—Å—Ç–µ–º–µ?", access_level: int = 50) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        logger.info(f"=== –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –†–ï–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø ===")
        logger.info(f"Query: {query}")
        logger.info(f"Access level: {access_level}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã
        database_service = DatabaseService()
        collection = database_service.get_collection()
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_docs = collection.get(
            where={"access_level": {"$lte": access_level}},
            limit=30,
            include=['documents']
        )
        
        if not test_docs['documents']:
            logger.error("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è!")
            return {"error": "No documents found for testing"}
        
        documents = test_docs['documents'][:20]  # –ë–µ—Ä–µ–º 20 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        
        results = {}
        
        # 1. –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.info("\n" + "="*50)
        results["isolated"] = self.profile_isolated_reranking(query, documents, iterations=3)
        
        # 2. –†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ pipeline
        logger.info("\n" + "="*50)
        results["pipeline"] = self.profile_pipeline_reranking(query, access_level, iterations=3)
        
        # 3. –ü–æ—à–∞–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        logger.info("\n" + "="*50)
        results["step_by_step"] = self.profile_step_by_step_pipeline(query, access_level)
        
        # 4. –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        isolated_avg = results["isolated"]["statistics"]["avg_time_ms"]
        pipeline_avg = results["pipeline"]["statistics"]["avg_time_ms"]
        rerank_time_in_pipeline = results["step_by_step"]["steps_timing_ms"]["total_reranking"]
        
        results["comparison"] = {
            "isolated_avg_ms": isolated_avg,
            "pipeline_avg_ms": pipeline_avg,
            "rerank_time_in_pipeline_ms": rerank_time_in_pipeline,
            "slowdown_factor_pipeline_vs_isolated": pipeline_avg / isolated_avg if isolated_avg > 0 else 0,
            "slowdown_factor_pipeline_rerank_vs_isolated": rerank_time_in_pipeline / isolated_avg if isolated_avg > 0 else 0,
            "rerank_percentage_of_pipeline": (rerank_time_in_pipeline / pipeline_avg) * 100 if pipeline_avg > 0 else 0
        }
        
        # 5. –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        results["analysis"] = self._analyze_results(results)
        
        return results
    
    def _analyze_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –≤—ã–≤–æ–¥—ã"""
        comparison = results["comparison"]
        step_by_step = results["step_by_step"]
        
        analysis = {
            "findings": [],
            "bottlenecks": [],
            "recommendations": []
        }
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–º–µ–¥–ª–µ–Ω–∏—è
        slowdown = comparison["slowdown_factor_pipeline_vs_isolated"]
        if slowdown > 10:
            analysis["findings"].append(f"–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –∑–∞–º–µ–¥–ª–µ–Ω–∏–µ: {slowdown:.1f}x –º–µ–¥–ª–µ–Ω–Ω–µ–µ –≤ pipeline")
        elif slowdown > 3:
            analysis["findings"].append(f"–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–º–µ–¥–ª–µ–Ω–∏–µ: {slowdown:.1f}x –º–µ–¥–ª–µ–Ω–Ω–µ–µ –≤ pipeline")
        
        # –ê–Ω–∞–ª–∏–∑ —É–∑–∫–∏—Ö –º–µ—Å—Ç
        steps_timing = step_by_step["steps_timing_ms"]
        total_time = steps_timing["total_pipeline"]
        
        for step, time_ms in steps_timing.items():
            if step != "total_pipeline" and (time_ms / total_time) > 0.5:  # –ë–æ–ª–µ–µ 50% –≤—Ä–µ–º–µ–Ω–∏
                analysis["bottlenecks"].append(f"{step}: {time_ms:.1f}ms ({(time_ms/total_time)*100:.1f}%)")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–∞–º—è—Ç–∏
        memory_deltas = step_by_step["memory_deltas_mb"]
        for step, delta in memory_deltas.items():
            if delta > 100:  # –ë–æ–ª–µ–µ 100MB
                analysis["bottlenecks"].append(f"–ü–∞–º—è—Ç—å {step}: +{delta:.1f}MB")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        rerank_percentage = comparison["rerank_percentage_of_pipeline"]
        if rerank_percentage > 80:
            analysis["recommendations"].append("–†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–Ω–∏–º–∞–µ—Ç >80% –≤—Ä–µ–º–µ–Ω–∏ - –∫—Ä–∏—Ç–∏—á–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å")
        
        predict_time = steps_timing.get("rerank_model_predict", 0)
        if predict_time > 20000:  # –ë–æ–ª–µ–µ 20 —Å–µ–∫—É–Ω–¥
            analysis["recommendations"].append("model.predict() –∑–∞–Ω–∏–º–∞–µ—Ç >20—Å–µ–∫ - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ")
        
        return analysis


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"""
    profiler = DetailedRerankingProfiler()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    results = profiler.run_comprehensive_analysis(
        query="–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –µ—Å—Ç—å –≤ —Å–∏—Å—Ç–µ–º–µ?",
        access_level=50
    )
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "="*80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê")
    print("="*80)
    
    comparison = results["comparison"]
    print(f"\nüìä –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: {comparison['isolated_avg_ms']:.1f}ms")
    print(f"Pipeline –ø–æ–ª–Ω–æ—Å—Ç—å—é: {comparison['pipeline_avg_ms']:.1f}ms")
    print(f"–†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ pipeline: {comparison['rerank_time_in_pipeline_ms']:.1f}ms")
    print(f"–ó–∞–º–µ–¥–ª–µ–Ω–∏–µ pipeline vs isolated: {comparison['slowdown_factor_pipeline_vs_isolated']:.1f}x")
    print(f"–î–æ–ª—è —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤ pipeline: {comparison['rerank_percentage_of_pipeline']:.1f}%")
    
    step_by_step = results["step_by_step"]
    print(f"\n‚è±Ô∏è –ü–û–®–ê–ì–û–í–û–ï –í–†–ï–ú–Ø:")
    for step, time_ms in step_by_step["steps_timing_ms"].items():
        if step != "total_pipeline":
            percentage = (time_ms / step_by_step["steps_timing_ms"]["total_pipeline"]) * 100
            print(f"  {step}: {time_ms:.1f}ms ({percentage:.1f}%)")
    
    analysis = results["analysis"]
    print(f"\nüîç –ê–ù–ê–õ–ò–ó:")
    for finding in analysis["findings"]:
        print(f"  ‚Ä¢ {finding}")
    
    print(f"\nüö® –£–ó–ö–ò–ï –ú–ï–°–¢–ê:")
    for bottleneck in analysis["bottlenecks"]:
        print(f"  ‚Ä¢ {bottleneck}")
    
    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    for recommendation in analysis["recommendations"]:
        print(f"  ‚Ä¢ {recommendation}")
    
    # –î–µ—Ç–∞–ª–∏ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    rerank_details = step_by_step["reranking_details"]
    print(f"\nüî¨ –î–ï–¢–ê–õ–ò –†–ï–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø:")
    print(f"  –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {step_by_step['documents_count']}")
    print(f"  –ü–∞—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {rerank_details['pairs_count']}")
    print(f"  –ü–∞–º—è—Ç—å –¥–æ predict: {rerank_details['memory_before_predict']['rss_mb']:.1f}MB")
    print(f"  –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ predict: {rerank_details['memory_after_predict']['rss_mb']:.1f}MB")
    print(f"  –ü—Ä–∏—Ä–æ—Å—Ç –ø–∞–º—è—Ç–∏: {rerank_details['memory_delta_predict_mb']:.1f}MB")
    print(f"  model.predict() –≤—Ä–µ–º—è: {step_by_step['steps_timing_ms']['rerank_model_predict']:.1f}ms")
    
    return results


if __name__ == "__main__":
    main()
